how you get system info using promethues
"To gather system information like CPU usage, we configure Prometheus to scrape metrics using node_exporter, which runs on the target system. This data is then visualized in Grafana for better monitoring and alerting."




Kubernetes Architecture
========================

+-------------------------------------------------------------+
|                        Kubernetes Cluster                    |
|                                                             |
|  +------------------------+     +------------------------+  |
|  |       Master Node        |     |       Master Node        |  |
|  |------------------------|     |------------------------|  |
|  | API Server  | etcd      |     | API Server  | etcd      |  |
|  | Scheduler   | Controller|     | Scheduler   | Controller|  |
|  +------------------------+     +------------------------+  |
|                |                             |                |
|                |                             |                |
|      +----------------------------+         |                |
|      |                            |         |                |
|      v                            v         v                |
|  +-------------------+   +-------------------+   +-------------------+ |
|  |    Worker Node 1    |   |    Worker Node 2    |   |    Worker Node 3    | |
|  |-------------------|   |-------------------|   |-------------------| |
|  | Kubelet  | Kube Proxy|   | Kubelet  | Kube Proxy|   | Kubelet  | Kube Proxy| |
|  | Runtime  |            |   | Runtime  |            |   | Runtime  |            | |
|  +-------------------+   +-------------------+   +-------------------+ |
|           |                             |                             |
|  +-------------------+   +-------------------+   +-------------------+ |
|  |       Pods          |   |       Pods          |   |       Pods          | |
|  | +--------+ +--------+ | | +--------+ +--------+ | | +--------+ +--------+ |
|  | |  Pod 1  | |  Pod 2  | | |  Pod 3  | |  Pod 4  | | |  Pod 5  | |  Pod 6  | |
|  | +--------+ +--------+ | | +--------+ +--------+ | | +--------+ +--------+ |
|  +-------------------+   +-------------------+   +-------------------+ |
|                                                             |
|                 +--------------------+                     |
|                 |      Networking     |                     |
|                 |--------------------|                     |
|                 | NodePort  | ClusterIP|                     |
|                 | LoadBalancer        |                     |
|                 +--------------------+                     |
|                                                             |
|  +------------------+   +-------------------+               |
|  |    ConfigMaps      |   |      Secrets        |               |
|  +------------------+   +-------------------+               |
|                                                             |
|  +------------------+   +-------------------+               |
|  |     Ingress        |   |      Egress         |               |
|  +------------------+   +-------------------+               |
+-------------------------------------------------------------+








Kubernetes Architecture
========================

                          +----------------------------+
                          |         Master Node         |
                          |----------------------------|
                          | API Server  | etcd          |
                          | Scheduler   | Controller    |
                          +----------------------------+
                                     |   |   |
                                     |   |   |
          +--------------------------+   |   +------------------------+
          |                              |                            |
+---------------------+    +---------------------+    +---------------------+
|      Worker Node 1   |    |      Worker Node 2   |    |      Worker Node 3   |
|---------------------|    |---------------------|    |---------------------|
| Kubelet  | Kube Proxy|    | Kubelet  | Kube Proxy|    | Kubelet  | Kube Proxy|
| Runtime  |            |    | Runtime  |            |    | Runtime  |            |
|---------------------|    |---------------------|    |---------------------|
|       Pods            |    |       Pods            |    |       Pods            |
|  +--------+ +--------+|    |  +--------+ +--------+|    |  +--------+ +--------+|
|  |  Pod 1  | |  Pod 2  |    |  |  Pod 3  | |  Pod 4  |    |  |  Pod 5  | |  Pod 6  |
|  +--------+ +--------+|    |  +--------+ +--------+|    |  +--------+ +--------+|
+---------------------+    +---------------------+    +---------------------+

Networking
==========
- NodePort: Exposes services on each Nodeâ€™s IP.
- ClusterIP: Exposes services internally within the cluster.
- LoadBalancer: Exposes services externally using a cloud provider.

Other Components
=================
- Namespace: Isolates groups of resources within the cluster.
- ConfigMaps: Manages configuration data for your applications.
- Ingress: Manages HTTP/HTTPS traffic to services within the cluster.
- Egress: Controls outbound traffic from pods.



Explanation of Key Components

1. Cluster
==========
A Kubernetes Cluster is the entire system that contains:
Master Nodes (for control and management)
Worker Nodes (for running application workloads)
Networking components like NodePort, ClusterIP, and LoadBalancer
Resources like ConfigMaps, Secrets, Ingress, and Egress.

2. Master Node
==============
Controls and manages the Kubernetes cluster.
Key Components:
API Server: The entry point for all REST commands sent to the cluster.
etcd: A distributed key-value store that holds cluster data.
Scheduler: Assigns pods to worker nodes.
Controller Manager: Ensures desired state is maintained (e.g., restarting pods if they fail).

3. Worker Node
==============
Executes the containerized applications.
Key Components:
Kubelet: Communicates with the master node and ensures pods are running.
Kube Proxy: Manages networking within and outside the cluster.
Container Runtime: Runs containers inside pods (e.g., Docker, containerd).

4. Pods
=======
The smallest deployable unit in Kubernetes.
Each pod can contain one or more containers sharing:
Networking (same IP address)
Storage Volumes

5. Networking
=============
NodePort: Exposes services on each nodeâ€™s IP at a static port.
ClusterIP: Exposes services internally within the cluster (default service type).
LoadBalancer: Exposes services externally using a cloud providerâ€™s load balancer.

6. Other Components
===================
Namespaces: Logical separation for different environments (e.g., Dev, QA, Prod).
ConfigMaps: Stores non-sensitive configuration data for pods.
Secrets: Stores sensitive information like passwords securely.
Ingress: Routes HTTP/HTTPS traffic into the cluster.
Egress: Manages outbound traffic from pods to external system


âœ… Cluster â†’ The entire Kubernetes system that includes Master Nodes, Worker Nodes, and other resources.

âœ… Node â†’ A machine (physical or virtual) inside the cluster.

A Worker Node runs application workloads.
Each Worker Node contains one or more Pods.
âœ… Pod â†’ The smallest deployable unit in Kubernetes.

Each Pod can contain one or more containers that share networking and storage.
ðŸ”Ž Key Clarification
A Node is not a container itself; itâ€™s a machine that runs Pods (which hold containers).
A Pod is designed to run closely related containers (like a main app and its sidecar for logging or monitoring).
Example:

Cluster â†’ Contains 3 Worker Nodes.
Worker Node 1 â†’ Has 2 Pods.
Pod 1 â†’ Contains 3 Containers (e.g., App, Logger, and Sidecar).


=====================================================================================================

Namespace in Kubernetes â€“ Clear Understanding
What is a Namespace?
A Namespace in Kubernetes is a way to logically divide a cluster into multiple virtual clusters. It is useful when you have multiple teams or applications running in the same Kubernetes cluster and need to keep resources organized.

Why Do We Need Namespaces?
Isolation: Different teams or environments (Dev, UAT, Prod) can work independently.
Resource Management: We can allocate specific CPU, memory, and storage limits per namespace.
Avoid Name Conflicts: Multiple applications can have the same resource names (e.g., Pods, Services) in different namespaces.
Access Control: Different teams can have different permissions within their namespaces.
How Namespaces Work
Kubernetes starts with a default namespace, where all resources are created if no namespace is specified.
Other built-in namespaces:
kube-system: System-related resources (like CoreDNS).
kube-public: Public resources accessible to all users.
kube-node-lease: Helps in node heartbeats.
You can create custom namespaces for better organization.


kubectl create namespace UAT
kubectl get namespaces  # Verify the namespace is created
kubectl apply -f uat-namespace.yaml
kubectl get namespaces  # Verify
kubectl apply -f uat-deployment.yaml
kubectl get pods -n UAT  # Verify Pods


kubectl get all -n UAT      ------ kubectl get all -n UAT


kubectl logs -n UAT -l app=uat-app  ----  Check Logs

curl http://uat.myapp.com    ----    Access Application


kubectl delete namespace UAT    --- 
  

==================================================================================================



ðŸ”¹ Foundational Concepts (Essentials)
1.	Cluster Architecture
o	Master Node, Worker Nodes, Pods, and Containers
o	Role of API Server, Controller Manager, Scheduler, and etcd
2.	Core Resources
o	Pods, Deployments, ReplicaSets, DaemonSets, StatefulSets
o	ConfigMaps, Secrets, Volumes, and PersistentVolumeClaims (PVC)
3.	Networking
o	ClusterIP, NodePort, and LoadBalancer services
o	Ingress Controller (e.g., NGINX, Traefik) for routing
o	DNS Management inside Kubernetes
4.	Storage Management
o	Persistent Volume (PV) and Persistent Volume Claims (PVC)
o	Dynamic and static storage provisioning
5.	Resource Management
o	CPU & Memory requests/limits
o	Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA)
________________________________________
ðŸ”¹ Intermediate Concepts (Crucial for Growth)
1.	Namespace Management
o	Isolating resources for better organization
2.	RBAC (Role-Based Access Control)
o	Creating Roles, ClusterRoles, and RoleBindings
3.	Networking Policies
o	Defining rules for pod-to-pod communication
o	Implementing egress and ingress policies
4.	Service Mesh
o	Using Istio, Linkerd, or Consul for traffic control, observability, and security
5.	Helm Charts
o	Packaging and deploying applications with Helm
o	Managing application versions effectively
________________________________________
ðŸ”¹ Advanced Concepts (Key for 5-Year Experience)
1.	CI/CD Integration
o	Automating deployments using GitLab CI, Jenkins, or ArgoCD
2.	Cluster Autoscaling
o	Configuring Cluster Autoscaler for scaling nodes dynamically
3.	Disaster Recovery (DR)
o	Backups for etcd and cluster data
o	Disaster recovery strategies for high availability
4.	Security Best Practices
o	Using NetworkPolicies for enhanced security
o	Implementing Pod Security Standards (PSS)
o	Image scanning, Secrets encryption, and Role-based controls
5.	Monitoring & Logging
o	Using Prometheus, Grafana, and ELK Stack for observability
o	Integrating Datadog, New Relic, or Splunk for performance tracking
6.	Multi-Cluster Management
o	Managing multiple clusters using Cluster API or KubeFed
________________________________________
ðŸ”¹ Real-World Scenarios (Highly Important for Interviews)
âœ… Blue-Green Deployment / Canary Deployment
âœ… Efficient use of Ingress Controllers for handling external traffic
âœ… Scaling strategies for large workloads
âœ… Troubleshooting skills â€” Logs, Events, and kubectl commands
Key Differences: HPA vs VPA
Feature	HPA	VPA
Scaling Type	Scales the number of pods	Adjusts resource requests/limits
Best Use Case	Apps with variable load (e.g., web servers)	Apps with constant load (e.g., batch jobs)
Recommended Mode	For stateless applications	For stateful applications


1. CPU & Memory Requests/Limits
In Kubernetes, each container can specify:
âœ… Requests: The minimum amount of CPU/Memory guaranteed to the container.
âœ… Limits: The maximum amount of CPU/Memory a container can consume.
Example YAML
apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
spec:
  containers:
  - name: sample-container
    image: nginx
    resources:
      requests:
        cpu: "250m"     # 250 millicores (0.25 CPU cores)
        memory: "256Mi" # 256 MiB of memory
      limits:
        cpu: "500m"     # 500 millicores (0.5 CPU cores)
        memory: "512Mi" # 512 MiB of memory
How It Works
â€¢	If the container tries to exceed the limit, it will be throttled for CPU or killed for exceeding memory.
â€¢	If the container does not get its request, the pod may not be scheduled.
________________________________________
2. Horizontal Pod Autoscaler (HPA)
HPA scales the number of pods based on CPU/Memory utilization or custom metrics.
Example HPA YAML
yaml
CopyEdit
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: example-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sample-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
How It Works
â€¢	In this example, HPA scales the pods between 2 and 10 replicas based on 70% CPU utilization.
â€¢	If CPU usage crosses 70%, new pods are added.
â€¢	If CPU usage drops below 70%, pods are removed.
Command to Apply HPA:
kubectl apply -f example-hpa.yaml
Command to Check HPA:
kubectl get hpa
________________________________________
3. Vertical Pod Autoscaler (VPA)
VPA adjusts the CPU/Memory requests and limits for existing pods to optimize resource usage.
Example VPA YAML
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: example-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sample-deployment
  updatePolicy:
    updateMode: "Auto"
How It Works
â€¢	VPA automatically adjusts requests/limits based on actual resource consumption.
â€¢	There are 3 modes for updateMode: 
o	Off â€“ No automatic updates, only recommendations.
o	Initial â€“ Resources are adjusted only during pod creation.
o	Auto â€“ Automatically adjusts requests/limits for running pods.
Command to Apply VPA:s
kubectl apply -f example-vpa.yaml
Command to Check VPA Recommendations:
kubectl get vpa


1. Cluster Architecture Overview
A Kubernetes cluster has two main parts:
Master Node (Control Plane)
Responsible for managing the entire cluster.
Component	Description
API Server	Central control point that handles all Kubernetes REST requests. Every kubectl command interacts with the API server.
Controller Manager	Monitors the cluster state and ensures desired conditions are maintained. Examples: Node controller, Deployment controller, etc.
Scheduler	Assigns new pods to suitable worker nodes based on available resources.
etcd	A distributed key-value store that stores cluster configuration and state data. Highly available and fault-tolerant.s
________________________________________
Worker Node
Responsible for running application workloads.
Component	Description
Kubelet	Ensures containers are running as defined in the PodSpec. Communicates with the API server.
Kube Proxy	Maintains network rules to enable communication between pods and services.
Container Runtime	Runs containers. Examples: Docker, containerd, or CRI-O.
________________________________________
Architecture Diagram (Text-Based)
pgsql
CopyEdit
+------------------------+       +------------------------+
|       Master Node        |       |      Worker Node 1      |
|------------------------ |       |------------------------ |
| API Server | etcd        |       | Kubelet   | Kube Proxy  |
| Controller | Scheduler   |       | Runtime   | Pods         |
+------------------------+       +------------------------+
                                       |
                                       |
                               +------------------------+
                               |      Worker Node 2      |
                               |------------------------ |
                               | Kubelet   | Kube Proxy  |
                               | Runtime   | Pods         |
                               +------------------------+
________________________________________
2. Core Resources
Kubernetes has key resources that manage application deployment, scaling, and updates.
Resource	Description
Pod	The smallest deployable unit in Kubernetes. Each pod can contain one or more tightly coupled containers.
Deployment	Manages multiple pod replicas, ensuring high availability and seamless updates.
ReplicaSet	Ensures the desired number of pod replicas are running. Used by Deployments.
DaemonSet	Ensures one pod runs on every node (ideal for logging, monitoring, etc.).
StatefulSet	Manages stateful applications, ensuring stable network identities and persistent storage.
________________________________________
3. Configuration Management
Resource	Description
ConfigMap	Stores non-sensitive data like environment variables and configuration files.
Secret	Stores sensitive data like passwords, tokens, and keys in an encrypted format.
Volume	Provides temporary or permanent storage for pods.
PersistentVolumeClaim (PVC)	A request for storage that pods use to claim storage from PersistentVolumes.
________________________________________
4. Networking in Kubernetes
Kubernetes networking ensures pods and services communicate securely.
Service Type	Description
ClusterIP	Exposes services internally within the cluster (default type).
NodePort	Exposes services on a specific node's IP and port. Ideal for debugging but less secure for production.
LoadBalancer	Exposes services externally via a cloud provider's load balancer (e.g., AWS ELB, GCP LB).
Ingress Controller	Routes HTTP/HTTPS traffic to services based on defined rules. Examples: NGINX, Traefik.
Ingress Example YAML
yaml
CopyEdit
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
________________________________________
5. Storage Management
Kubernetes supports two types of storage provisioning:
Persistent Volume (PV)
â€¢	A physical storage unit (e.g., NFS, AWS EBS, Azure Disk) provisioned by admins.
Persistent Volume Claim (PVC)
â€¢	A request for storage by a user. PVC binds to an available PV.
Example PV and PVC YAML
Persistent Volume (PV)
yaml
CopyEdit
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
Persistent Volume Claim (PVC)
yaml
CopyEdit
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: example-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
________________________________________
6. Storage Provisioning
Provisioning Type	Description
Static Provisioning	Admins manually create Persistent Volumes (PVs) before they are claimed.
Dynamic Provisioning	Kubernetes automatically provisions storage when PVC requests are made (e.g., AWS EBS, GCE PD).
________________________________________
Summary Workflow
âœ… Master Node controls and manages the cluster.
âœ… Worker Nodes run application workloads.
âœ… Pods are the smallest deployable units.
âœ… Deployments ensure scalability and rolling updates.
âœ… ConfigMaps & Secrets manage configuration securely.
âœ… ClusterIP, NodePort, and LoadBalancer provide networking options.
âœ… PV/PVC handles data persistence.


Namespace Management
Namespaces in Kubernetes help isolate resources for better organization and access control.
â€¢	Purpose: Useful for dividing cluster resources among multiple teams or projects.
â€¢	Example: Dev, UAT, and Prod environments can each have separate namespaces.
Command to Create a Namespace:
bash
CopyEdit
kubectl create namespace dev
________________________________________
RBAC (Role-Based Access Control)
RBAC controls who can access Kubernetes resources and what actions they can perform.
âœ… Role: Defines permissions within a specific namespace.
âœ… ClusterRole: Defines permissions across all namespaces.
âœ… RoleBinding: Binds a Role to a user or group.
âœ… ClusterRoleBinding: Binds a ClusterRole to users across the cluster.
Example Role YAML
yaml
CopyEdit
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: developer-role
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "create"]
________________________________________
Networking Policies
Network policies control pod-to-pod communication for security and isolation.
âœ… Ingress Rules: Control incoming traffic to pods.
âœ… Egress Rules: Control outgoing traffic from pods.
Example NetworkPolicy YAML
yaml
CopyEdit
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-internal
  namespace: dev
spec:
  podSelector:
    matchLabels:
      role: backend
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend
________________________________________
Service Mesh
A service mesh manages communication between microservices, providing:
âœ… Traffic Control â€“ Load balancing, retries, and failover.
âœ… Observability â€“ Metrics, tracing, and logging.
âœ… Security â€“ Mutual TLS (mTLS) for encrypted traffic.
Popular Service Mesh Tools:
â€¢	Istio â€“ Advanced traffic management and observability.
â€¢	Linkerd â€“ Lightweight and simple service mesh.
â€¢	Consul â€“ Focuses on multi-cloud networking.
________________________________________
Helm Charts
Helm is a package manager for Kubernetes that simplifies application deployment.
âœ… Helm Chart: A package of Kubernetes manifests.
âœ… Values.yaml: Configuration file for customization.
âœ… Release Management: Helps manage different versions of deployed applications.
Install Example Using Helm:
bash
CopyEdit
helm install my-app ./my-chart
Upgrade Application:
bash
CopyEdit
helm upgrade my-app ./my-chart
________________________________________
Summary
â€¢	Namespaces improve organization and isolate resources.
â€¢	RBAC controls user permissions in a secure way.
â€¢	Networking Policies manage pod communication.
â€¢	Service Mesh enhances traffic control and security.
â€¢	Helm Charts simplify deployment and version control.
==================================================================================
1. CI/CD Integration
Integrating Kubernetes with CI/CD tools automates deployment processes for faster, reliable updates.
âœ… GitLab CI, Jenkins, and ArgoCD are commonly used.
âœ… These tools automate building, testing, and deploying applications to Kubernetes clusters.
Example GitLab CI/CD Pipeline for Kubernetes Deployment:
yaml
CopyEdit
stages:
  - build
  - deploy

deploy:
  stage: deploy
  script:
    - kubectl apply -f deployment.yaml
    - kubectl apply -f service.yaml
________________________________________
2. Cluster Autoscaling
The Cluster Autoscaler automatically adjusts the number of nodes based on resource demands.
âœ… Adds nodes when pods canâ€™t be scheduled due to resource limits.
âœ… Removes nodes when resources are underutilized.
Key Configurations:
â€¢	Requires cloud providers like AWS, GCP, or Azure to manage scaling.
â€¢	Configured via cluster-autoscaler deployment.
Command to Enable Autoscaler on AWS EKS:
bash
CopyEdit
eksctl create cluster --name=my-cluster --nodes-min=2 --nodes-max=5
________________________________________
3. Disaster Recovery (DR)
Disaster recovery strategies ensure data protection and high availability.
âœ… Backup etcd: Since etcd stores cluster state, its backup is crucial.
âœ… Tools like Velero, Kasten, or native etcd snapshots are used.
Backup etcd Data:
bash
CopyEdit
ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-backup.db
Restore etcd Data:
bash
CopyEdit
ETCDCTL_API=3 etcdctl snapshot restore /backup/etcd-backup.db
Disaster Recovery Strategy:
â€¢	Maintain multiple replica control planes.
â€¢	Distribute nodes across multiple availability zones for redundancy.
________________________________________
4. Security Best Practices
To secure your cluster:
âœ… Network Policies: Control pod communication.
âœ… Pod Security Standards (PSS): Enforces security contexts in pods.
âœ… Image Scanning: Tools like Trivy, Aqua Security, or Clair scan container images.
âœ… Secrets Encryption: Encrypt Kubernetes secrets using KMS (Key Management Service).
Example Network Policy for Secure Traffic:
yaml
CopyEdit
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-external-traffic
spec:
  podSelector: {}
  ingress: []
________________________________________
5. Monitoring & Logging
Monitoring ensures visibility into cluster performance, while logging helps diagnose issues.
âœ… Prometheus & Grafana â€“ Metrics collection and visualization.
âœ… ELK Stack (Elasticsearch, Logstash, Kibana) â€“ Centralized logging.
âœ… Datadog, New Relic, or Splunk â€“ Advanced monitoring solutions.
Prometheus Sample Alert Rule:
yaml
CopyEdit
groups:
- name: example
  rules:
  - alert: HighCPUUsage
    expr: node_cpu_seconds_total > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage detected"
________________________________________
6. Multi-Cluster Management
Managing multiple clusters ensures better scalability and fault tolerance.
âœ… Cluster API (CAPI): Automates cluster lifecycle management.
âœ… KubeFed (Kubernetes Federation): Manages multiple clusters under one control plane.
________________________________________
Real-World Scenarios (Interview Focus)
âœ… Blue-Green Deployment:
â€¢	Deploy new versions alongside existing ones and switch traffic gradually.
â€¢	Ensures zero downtime during upgrades.
âœ… Canary Deployment:
â€¢	Gradually roll out changes to a subset of users before full deployment.
âœ… Ingress Controller Optimization:
â€¢	Use tools like NGINX, Traefik, or HAProxy for load balancing and traffic management.
âœ… Scaling Strategies:
â€¢	Use HPA (Horizontal Pod Autoscaler) for app scaling.
â€¢	Combine with Cluster Autoscaler for dynamic node scaling.
âœ… Troubleshooting Skills:
Use these essential commands for debugging:
â€¢	kubectl logs <pod> â†’ View container logs
â€¢	kubectl describe pod <pod> â†’ Detailed pod information
â€¢	kubectl get events â†’ Identify cluster issues
â€¢	kubectl exec -it <pod> -- /bin/sh â†’ Access pod terminal for live debugging
Key Difference: CAPI vs KubeFed
Feature	Cluster API (CAPI)	KubeFed
Purpose	Creates and manages clusters from scratch	Manages existing clusters collectively
Best Use Case	Provisioning clusters in cloud/on-prem environments	Synchronizing resources between clusters
Automation	Full automation (creation, scaling, deletion)	Manages resources centrally (but doesn't create clusters)
________________________________________
ðŸš€ Recommended Approach for You
â€¢	Use Cluster API if you need to create and manage clusters efficiently.
â€¢	Use KubeFed if you already have multiple clusters and need centralized control.
Combining both is powerful: CAPI for creation + KubeFed for unified management.



